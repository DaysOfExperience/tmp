# 多路复用IO

## 多路复用IO的概念

**多路复用IO用于<u>对大量描述符进行IO就绪事件监控</u>，<u>能够让用户只针对就绪了指定事件的描述符进行操作</u>。**相比较于其他IO方式，多路复用IO **避免了 对没有就绪的描述符进行操作而带来的阻塞，只针对已就绪的描述符进行操作，提高了效率.**

**IO的就绪事件分为可读、可写、异常**

> - **可读事件**：一个描述符对应的接收缓冲区中有数据可读
> - **可写事件**：一个描述符对应的发送缓冲区中有剩余空间可以写入数据
> - **异常事件**：一个描述符发生了特定的异常信息

在Linux下，操作系统针对多路复用IO提供了三种模型：**select**模型、**poll**模型、**epoll**模型。

## 多路复用IO与多线程/多进程的并发

### 多路复用IO模型进行服务器并发处理

即在**单执行流中进行轮询处理就绪的描述符**。

缺点是：如果就绪的描述符较多时，很难做到负载均衡（最后一个描述符要等待很长时间，前边的描述符处理完了才能处理它）。

解决这一问题的方法就是在**用户态实现负载均衡**，即规定每个描述符只能读取指定数量的数据，读取了就进行下一个描述符。

**多路复用IO模型适用于有大量描述符需要监控，但是同一时间只有少量活跃的场景**。

### 多线程/多进程进行服务器并发处理（？）

即操作系统通过轮询调度执行流，实现每个执行流中描述符的处理
由于其在**内核态实现了负载均衡**，所以不需要用户态做过多操作

**多路复用适合于IO密集型服务，多进程或线程适合于CPU密集型服务**，它们各有各的优势，并不存在谁取代谁的倾向。基于两者的特点，通常可以将多路复用IO和多线程/多进程搭配一起使用：使用多路复用IO监控大量的描述符，哪个描述符有事件到来，就创建执行流去处理。这样做的好处是防止直接创建执行流而描述符还未就绪，浪费资源。

# select

[select](https://github.com/DaysOfExperience/Linux_Network_Programming/tree/main/IO_Multiplexing)

## 工作原理（步骤）

1. 定义指定监控事件的描述符集合（即位图，fd_set类型），初始化集合（ FD_ZERO(&fs); // 对文件描述符集进行清空），将需要监控指定事件的描述符添加到指定事件（可读、可写、异常）的描述符集合中（添加到这个fd_set中：可以用一个数组维护需要监控的文件描述符，然后遍历它，添加到fd_set中）

2. 调用select进行若干个文件描述符的事件监控，内部过程为：**将描述符集合fd_set类型对象拷贝到内核当中，对集合中所有描述符进行轮询判断，当描述符就绪或者等待超时后就调用返回**（select返回），**返回后的集合中只剩下已就绪的描述符**（未就绪会在位图中置为0）

3. 通过遍历描述符集合fd_set类型对象，判断哪些描述符还在集合中，就可以知道哪些描述符已经就绪了，开始处理对应的IO事件。

## 接口

```C++
//清空集合
void FD_ZERO(fd_set *set);

//向集合中添加描述符fd
void FD_SET(int fd, fd_set *set);

//从集合中删除描述符fd
void FD_CLR(int fd, fd_set *set);

//判断描述符是否还在集合中
int  FD_ISSET(int fd, fd_set *set);   // select调用返回之后进行.

//发起select调用：将集合拷贝到内核中并进行监控
int select(int nfds, fd_set *readfds, fd_set *writefds,
                  fd_set *exceptfds, struct timeval *timeout);
// 所以其实我的代码中只是对读事件进行了监控，所以34参数为nullptr，5为nullptr是因为阻塞式。
/*
	fd:文件描述符
	set:描述符位图
	nfds:集合中最大描述符数值+1
	readfds:可读事件集合
	writefds:可写事件集合
	exceptfds:异常事件集合
	
	timeout:超时等待时间，传nullptr表示阻塞式select
	timeval结构体有两个成员
	struct timeval {
           long    tv_sec;    毫秒
           long    tv_usec;   微秒
    };
*/
```

## 优缺点

缺点：

1. select所能**监控的描述符数量有上限**，由宏__FD_SETSIZE决定，默认是1024个
2. **select会将集合拷贝到<u>内核中轮询遍历判断描述符是否就绪</u>，效率会随着描述符的增多而越来越低**
3. select监控完毕后返回的集合中只有已就绪的描述符，移除了未就绪的描述符，所以**每次监控都必须要重新将要监控的描述符加入集合中**，重新拷贝到内核
4. select返回的集合是一个位图而不是真正的就绪的描述符数组，所以**需要用户遍历判断哪个描述符在集合中才能确认其是否就绪**

```C++
            for (int i = 0; i < NUM; ++i)
            {
                if (_sock_array[i] != FD_NONE)
                {
                    if (_sock_array[i] > max_fd)
                        max_fd = _sock_array[i];
                    FD_SET(_sock_array[i], &fs); // 对文件描述符进行关心:在fd_set类型对象中进行设定
                }
            }        

		// fs中存储着当前读事件就绪的文件描述符
        for (int i = 0; i < NUM; ++i)
        {
            if (_sock_array[i] == FD_NONE)   // 对于这个套接字文件描述符根本不关心的
                continue;
            if (FD_ISSET(_sock_array[i], &fs))   // 关心这个文件描述符，且事件就绪了。
            {
                // 该文件描述符读事件就绪了，可以读了
                if (_sock_array[i] == _listen_sock)
                    Accepter();
                else
                    Recver(i); // 某文件描述符的读事件就绪了
            }
        }
```

**优点**：

1. select遵循posix标准，**可以跨平台移植**
2. select的超时等待时间较为精确，**可以精细到微秒**

# poll

[poll](https://github.com/DaysOfExperience/Linux_Network_Programming/tree/main/IO_Multiplexing/poll)

和select很像，select是每次初始化一个fd_set类型的位图对象，也就是要监控的描述符。（读写异常是不同的fd_set），然后select进行监控，返回之后，fd_set只有就绪的文件描述符，未就绪的就从这个fd_set里删除了。

而poll是维护一个<u>结构体数组</u>，pollfd []，依旧是把要关心的文件描述符和要关心的事件用一个pollfd维护起来，调用poll进行事件监控，当poll返回，pollfd.revents会表征这个文件描述符的什么事件就绪了，若为0就是没有就绪。

select返回，对fd_set进行遍历，poll返回，对pollfd[]进行遍历。

## 工作原理

1. 开辟pollfd结构体 数组，将需要监控的描述符以及监控的事件信息添加进去
2. 发起监控调用poll，将数组中的数据拷贝到**<u>内核当中进行轮询遍历监控</u>**，当有描述符就绪或者等待超时后返回，返回时将已就绪的事件添加进pollfd结构体中的revents中（如果没就绪，revents为0）
3. 监控调用返回后，遍历pollfd数组中的每一个节点的revents，根据对应的就绪事件进行相应操作

## 接口

```C++
struct pollfd 
{
	int   fd;         //需要监控的文件描述符
	short events;     //需要监控的事件
	short revents;    //实际就绪的事件
};
/*
	操作相对简单，如果某个描述符不需要继续监控时，直接将对应结构体中的fd置为-1即可。
*/


//发起监控
int poll(struct pollfd *fds, nfds_t nfds, int timeout);
/*
	fds:pollfd数组
	nfds:数组的大小
	timeout:超时等待时间，单位为毫秒
*/
```

## 优缺点

缺点：

1. **在内核中轮询判断描述符是否有事件就绪，效率会随着描述符的增加而下降**（select一样）
2. 每次调用返回后需要**用户自行判断revents才能知道是哪个描述符就绪了哪个事件**（select一样）
3. **无法跨平台移植**（select的优点）
4. **超时等待时间只精确到毫秒**（select的优点）

优点：

1. poll通过描述符事件结构体的方式将select的描述符集合的操作流程合并在一起，简化了操作（相比于select简化了一些操作）
   不需要每次调用poll进行事件监控都重新定义并初始化事件结构体数组（对比select，select需要在select监控前重新定义并维护fd_set)
2. poll所**监控的描述符数量没有限制**，需要多少描述符就给多大的数组（对比select，select有上限)

# epoll(event poll)

## 工作原理

```C++
struct eventpoll{  
   ....  
   /*红黑树的根节点，这颗树中存储着所有添加到epoll中的需要监控的事件*/  
   struct rb_root rbr;  
   /*双链表中则存放着将要通过epoll_wait返回给用户的满足条件的事件*/  
   struct list_head rdlist;  
   ....  
};
```

![image-20231121155519590](https://cdn.jsdelivr.net/gh/DaysOfExperience/blogImage@main/img/image-20231121155519590.png)

1. 在内核中创建eventpoll结构体，返回一个描述符作为操作句柄。

2. 对需要监控的描述符组织事件结构体(    struct epoll_event ev;

   ​    ev.events = events;

   ​    ev.data.fd = fd;

   )，将描述符和对应事件（这个epoll_event结构体）添加到内核的eventpoll结构体中。

3. **开始监控，epoll的监控是一个异步阻塞操作，他只需要告诉操作系统哪些描述符需要监控，然后这个监控的过程就由系统来完成。<u>操作系统为每一个描述符所需要的事件设置了一个回调函数，一旦对应事件就绪，就会自动调用回调函数，将描述符所对应的epoll_event事件结构体添加到rdllist双向链表中</u>  <u>发起监控后</u>，每隔一段时间就会去查看双向链表rdllist是否为空（阻塞操作, 除非链表不为空或者超时才会返回），<u>如果不为空则代表有描述符的事件就绪，将就绪的描述符的结构信息(epoll_event)添加到epoll_wait传入的events数组中（struct epoll_event *_revs;）</u>。而epoll_wait返回后, 用户只需要对events数组进行遍历，判断就绪的是什么事件然后对描述符进行相应处理即可。**

> 有个细节是通过epoll_ctl添加事件关心时是通过epoll_event.events数据成员告知要关心什么事件。获取就绪事件时也是通过epoll_event.events数据成员获取就绪了什么事件，不同于poll的struct pollfd的events 和 revents。主要是因为epoll直接通过内核的红黑树就组织了需要关心的事件。

## 接口

```C++
//在内核中创建eventpoll结构体，返回操作句柄（size为监控的最大数量，但是在linux2.6.8后忽略上限，只需要给一个大于0的数字即可）
int epoll_create(int size);

//组织描述符事件结构体
int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);
/*
	epfd:eventpoll结构体的操作句柄
	op:操作的选项，EPOLL_CTL_ADD/EPOLL_CTL_MOD/EPOLL_CTL_DEL
	fd:描述符
	event:监控描述符对应的事件信息结构体
	struct epoll_event 
	{
           uint32_t     events;    // 要监控的事件，以及调用返回后实际就绪的事件
           epoll_data_t data;      // 联合体，用来存放各种类型的描述符，主要是data.fd数据成员
	};
	typedef union epoll_data {
           void    *ptr;
           int      fd;
           uint32_t u32;
           uint64_t u64;
	} epoll_data_t;
*/

//开始监控，当有描述符就绪或者等待超时后调用返回
int epoll_wait(int epfd, struct epoll_event *events,
                      int maxevents, int timeout);
/*
	maxevents:events数组的结点数量
	timeout:超时等待时间
	返回值为就绪的描述符个数，在events指向数组的前n个元素中
*/
```

## 优缺点

**epoll是Linux下性能最高的多路复用IO模型，几乎具备了一切所需的优点**

缺点：

1. 无法跨平台移植
2. 超时等待时间只精确到毫秒（poll也有这两个缺点, 也是select唯二的优点)
3. **<u>在活动连接较多的时候，由于会大量触发回调函数(大量文件描述符的事件就绪)，此时epoll的效率未必会比select和poll高</u>**
   **<u>所以epoll适用于连接数量多，但是活动连接少的情况</u>**（重点！！！）（epoll的适用场景）

优点：

1. **底层用的是红黑树存储，监控的描述符数量没有上限**（select有上限，poll无上限
2. **所有的描述符事件信息只需要向内核中拷贝一次**（select和poll每次调用select和poll都需要向内核拷贝一次）
3. **监控采用异步阻塞，使用回调函数的方式，性能不会随着描述符增多而下降** (select和poll, 都会随着监控的描述符增多而性能下降, 因为内核是通过循环遍历的方式去检查是否有描述符的事件就绪)
4. **若有描述符的事件就绪, epoll_wait返回后, struct epoll_event *events数组中就绪描述符事件信息，可以直接对就绪描述符进行操作** (select和poll在返回后都需要遍历判断)

## epoll的工作模式

epoll有两种工作模式，**LT模式（水平触发模式）和ET模式（边缘触发模式）**。

### LT模式（水平触发）

LT模式也就是水平触发模式 - Level-Triggered，是epoll的默认触发模式**（select和poll只有这种模式）**

**触发条件**
可读事件：接受缓冲区中的数据大小高于低水位标记，则会触发事件
可写事件：发送缓冲区中的剩余空间大小大于低水位标记，则会触发事件
低水位标记：一个基准值，默认是1

所以简单点说，LT水平触发模式就是: **只要接收缓冲区中还有数据，就会一直触发可读事件；只要发送缓冲区中还有空间，就会一直触发可写事件。**

因此

- **当epoll检测到socket上事件就绪的时候, 可以不立刻进行处理或者只处理一部分**：如上面的例子, 由于只读了1K数据, 缓冲区中还剩1K数据, 在第二次调用 epoll_wait 时, epoll_wait 仍然会立刻返回并通知socket读事件就绪. 直到缓冲区上所有的数据都被处理完, epoll_wait 才不会立刻返回.

- 支持阻塞读写和非阻塞读写（效果都一样，因为只有接收缓冲区中有数据可读了才会触发读事件进行读取，此时阻塞读和非阻塞读都不会阻塞）

### ET模式（边缘触发）

ET模式也就是边缘触发模式 - Edge-Triggered

**触发条件**  : 在ET模式下，`epoll_wait` 只会在文件描述符状态发生变化的时候通知应用程序。

对于读操作

**(1) 当buffer由不可读状态变为可读的时候，即 由空变为不空的时候。**

**(2) 当有新数据到达时，即buffer中的待读内容变多的时候。**

<img src="https://cdn.jsdelivr.net/gh/DaysOfExperience/blogImage@main/img/image-20231121161600304.png" alt="image-20231121161600304" style="zoom:67%;" />

对于写操作

1. **当buffer由不可写变为可写的时候，即由满状态变为不满状态的时候。**

2. **当有旧数据被发送走时，即buffer中待写的内容变少得时候。**

![image-20231121161701662](https://cdn.jsdelivr.net/gh/DaysOfExperience/blogImage@main/img/image-20231121161701662.png)

**ET模式需要应用程序确保在文件描述符就绪时，必须读或写足够的数据，以免错过事件通知。**

<u>简单点说，ET模式下只有在新数据到来的情况下才会触发事件。这也就要求我们在新数据到来的时候必须一次性将所有数据取出，否则余留数据在下次epoll_wait时, 不会触发第二次事件，只有等到再有新数据到来才会触发。</u>
<u>而若想一次性处理完所有数据, 我们也不知道具体有多少数据，所以就需要循环处理，直到缓冲区为空，但是recv默认为阻塞读取，如果没有数据时就会阻塞等待，因此ET模式下需要将描述符的属性设置为非阻塞，才能解决这个问题。</u>（非阻塞读取）

```C++
void SetNoBlock(int fd) 
{
    int flag = fcntl(fd, F_GETFL);

    flag |= O_NONBLOCK;
    fcntl(fd, F_SETFL, flag);
}
```

> - **ET的性能比LT性能更高**( epoll_wait 返回的次数少了很多). Nginx默认采用ET模式的epoll模型.
> - ET只支持非阻塞的读写

### LT水平触发与ET边缘触发

所以简单点说，LT就是只要缓冲区中还有数据，就会一直触发事件，而ET模式下只有在新数据到来的情况下才会触发事件（或者发送缓冲区空间变化（剩余空间变大））。

LT模式的优点主要在于其简单且稳定，不容易出现问题，传统的select和poll都是使用这个模式。但是他也有缺点，就是因为事件触发过多导致效率降低。**<u>（LT简单稳定，但是效率低）</u>**
<u>ET最大的优点就是减少了epoll的触发次数（epoll_wait的返回次数）</u>，但是这也带来了巨大的代价，就是要求必须一次性将所有的数据处理完，虽然效率得到了提高，但是代码的复杂程度大大的增加了。Nginx就是默认采用ET模式。**<u>（ET效率高，但是代码复杂）</u>**

使用 ET 能够减少 epoll 触发的次数. 因为ET会**倒逼程序猿**事件就绪时就把所有的数据都处理完. **但是在 LT 模式下如果也能做到每次就绪的文件描述符都立刻处理并处理完全, 不让这个就绪事件被重复提示的话, LT性能和ET性能就一样了.** 另一方面, ET 的代码复杂程度更高了.

> 还有一种场景适合ET模式使用，如果我们需要接收一条数据，但是这条数据因为某种问题导致其发送不完整，需要分批发送。所以此时的缓冲区中数据只有部分，如果此时将其取出，**则会增加维护数据的开销**，所以正确的做法应该是等待后续数据到达后将其补全，再一次性取出。但是如果此时使用的是LT模式，就会因为接收缓冲区有数据(不为空)而一直触发事件，所以这种情况下使用ET会比较好。
> 也就是读事件触发，去检测接收缓冲区是否数据完整，不完整则不取出数据，下次epoll_wait并不会立刻返回，因为是ET模式。当接收缓冲区中数据完整了再一次性取出。

> 补充：为什么ET模式的epoll必须采用非阻塞读写 (其实挺好理解的，只是这里再举一个例子
>
> 使用 ET 模式的 epoll, 需要将文件描述设置为非阻塞. 这个不是接口上的要求, 而是 "工程实践" 上的要求.
>
> 假设这样的场景: 每当服务器接受到一个10k的请求, 就会向客户端返回一个应答. 如果客户端收不到应答, 不会发送第二个10k请求.
>
> 如果服务端是ET模式且使用阻塞式的read, 并且一次只 read 1k 数据的话(read不能保证一次就把所有的数据都读出来, 参考 man 手册的说明, 可能被信号打断), 剩下的9k数据就会待在缓冲区中.
> 此时由于 epoll 是ET模式, 故第二次epoll_wait不会认为文件描述符读就绪. epoll_wait 就不会再次返回. 剩下的 9k 数据会一直在缓冲区中. 直到下一次客户端再给服务器写数据. epoll_wait 才能返回
>
> 但是问题来了.
> 服务器只读到1k个数据, 要10k读完才会给客户端返回响应数据. 客户端要读到服务器的响应, 才会发送下一个请求；客户端发送了下一个请求，epoll_wait 才会返回, 才能去读缓冲区中剩余的数据.
>
> 所以, 为了解决上述问题 : 阻塞read不一定保证把完整的请求读完, 因此必须使用**非阻塞轮询**的方式来读取接收缓冲区, **保证一定能把完整的请求都读出来.**
>
> 而如果是LT则没这个问题, 因为只要接收缓冲区中的数据没读完, epoll_wait就会返回

### epoll LT模式实现TCP服务器

### epoll ET模式实现TCP服务器

[epoll-ET](https://github.com/DaysOfExperience/Linux_Network_Programming/tree/main/IO_Multiplexing/Reactor)

写的还不错的，封装Connection里面有应用层的接收和发送缓冲区，还有各种事件的回调处理方法。

重点是去理解这个ET模式的epoll对于recv send accept的影响，也就是accept要把所有建立好的连接全接收完。recv要把接收缓冲区读完。send要发送完应用层发送缓冲区为止，如果tcp发送缓冲区满了，就等。此时不要关闭写事件关心，这样下次tcp发送缓冲区空间变化了（ET）就会触发写事件就绪，进而继续发送应用层发送缓冲区中没有发送完的数据。

上方这段可以说是和ET模式强相关的地方了，还有就是在epoll_ctl添加关心时要在events里添加EPOLLET，并且要把所有的文件描述符设置为非阻塞！因为ET只能非阻塞！因为要循环处理！

## 惊群问题

惊群效应（thundering herd）是指多进程（多线程）在同时阻塞等待同一个事件的时候（休眠状态），如果等待的这个事件发生，那么他就会唤醒等待的所有进程（或者线程），但是最终却只能有一个进程（线程）获得这个事件的“控制权”，对该事件进行处理，而其他进程（线程）获取“控制权”失败，只能重新进入休眠状态，这种现象和性能浪费就叫做惊群效应。

---

所以当多个执行流同时在等待就绪事件时，**如果某个描述符就绪，他就会唤醒全部执行流中的epoll进行争抢，但是此时就只会有一个执行流抢到并执行**，而此时其他的执行流都会因为争抢失败而报错，错误码EAGAIN。这就是**惊群问题**。

**惊群问题带来了什么坏处呢？**

1. 一个就绪事件唤醒了多个执行流，多个执行流争抢资源，而最终只有一个能够成功，**导致了操作系统进行了大量无意义的调度、上下文切换，导致性能下降。**
2. 为了保证线程安全的问题，需要对资源进行加锁保护，增大了系统的开销

Linux 内核对用户进程（线程）频繁地做无效的调度、上下文切换等使系统性能大打折扣。

为了确保只有一个进程（线程）得到资源，需要对资源操作进行加锁保护，加大了系统的开销。目前一些常见的服务器软件有的是通过锁机制解决的，比如 Nginx（它的锁机制是默认开启的，可以关闭）；还有些认为惊群对系统性能影响不大，没有去处理，比如 Lighttpd。

### 多线程环境下惊群问题的解决方法

这种方法其实也就是本篇博客开头提到的一种做法。**只使用一个线程进行事件的监控，每当有就绪事件到来时，就将这些事件转交给其他线程去处理**，这样就避免了因为多执行流同时使用epoll监控而带来的惊群问题。

### 多进程环境下惊群问题的解决方法

这里主要借鉴的是lighttpd和nginx的解决方法。

lighttpd的解决思路很简单粗暴，就是直接无视这个问题，事件到来后依旧能够唤醒多个进程来争抢，并且只有一个能成功，其他进程争抢失败后的报错EAGAIN会被捕获，捕获后不会处理这个错误，而是直接无视，就当做没有发生。

nginx的解决思路是其实就是加锁与负载均衡。使用一个全局的互斥锁，每当有描述符就绪，就会让每个进程都去竞争这把锁（如果某个进程当前连接数达到了最大连接数的7/8，也就是其负载均衡点，此时这个进程就不会再去争抢所资源，而是将负载均衡到其他进程上），如果成功竞争到了锁，则将描述符加入进自己的wait集合中，而对于没有竞争到锁的进程，则将其从自己的wait集合中移除，这样就保证了不会让多个进程同一时间进行监控，而是让每个进程都通过竞争锁的方式轮流进行监控，这样保证了同一时间只会有一个进程进行监控，所以惊群问题也得到了解决。